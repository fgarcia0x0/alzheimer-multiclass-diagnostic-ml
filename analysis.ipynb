{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass diagnosis of Alzheimer's disease stages based on multimodal data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, make_scorer, accuracy_score, f1_score, precision_score, recall_score, auc, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_df_info(df):\n",
    "    print(f'df shape: {df.shape}')\n",
    "    #print(f'df info: {df.info()}')\n",
    "    print(f'Number os NANs: {df.isnull().sum().sum()}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = pd.read_csv(\"resource/patients.csv\")\n",
    "df_csf_source = pd.read_csv(\"resource/csf_data_roche_elecsys.csv\")\n",
    "df_mri_source = pd.read_csv(\"resource/ASHS volume data [ADNI2,3].csv\") # ASHS volume data [ADNI2,3]\n",
    "df_pet_source = pd.read_csv(\"resource/UCBERKELEY_AMY_6MM_07Aug2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csf = df_csf_source[[\"RID\", \"VISCODE2\", \"ABETA42\", \"TAU\", \"PTAU\"]]\n",
    "print_df_info(df_csf)\n",
    "\n",
    "print(round(df_csf.isnull().sum() / len(df_csf) * 100, 2))\n",
    "\n",
    "df_csf = df_csf.dropna(axis=0) # remove entire column\n",
    "print_df_info(df_csf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MRI Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mri = df_mri_source.iloc[:, 5:-1]\n",
    "df_mri.insert(0, 'RID', df_mri_source['RID'])\n",
    "df_mri.insert(1, 'VISCODE', df_mri_source['VISCODE'])\n",
    "\n",
    "print_df_info(df_mri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PET Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pet = df_pet_source.iloc[:, 11:-1]\n",
    "col_index = df_pet.columns.get_loc('AMYLOID_STATUS_COMPOSITE_REF') # get the index of AMYLOID_STATUS_COMPOSITE_REF\n",
    "\n",
    "df_pet.insert(0, 'RID', df_pet_source['RID'])\n",
    "df_pet.insert(1, 'VISCODE', df_pet_source['VISCODE'])\n",
    "print_df_info(df_pet)\n",
    "\n",
    "print(round(df_pet.isnull().sum()/len(df_pet)*100, 2))\n",
    "\n",
    "# handle missing data for categorical input\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_pet['AMYLOID_STATUS'] = cat_imputer.fit_transform(df_pet[['AMYLOID_STATUS']])\n",
    "df_pet['AMYLOID_STATUS_COMPOSITE_REF'] = cat_imputer.fit_transform(df_pet[['AMYLOID_STATUS']])\n",
    "\n",
    "# handle missing data for numerical input\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "for col in df_pet.columns[col_index + 1:]:\n",
    "    df_pet[col] = num_imputer.fit_transform(df_pet[[col]])\n",
    "\n",
    "# print info\n",
    "print_df_info(df_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge csf + patients\n",
    "df_merged = pd.merge(df_patients[['PHASE', 'RID', 'VISCODE', 'VISCODE2', 'DIAGNOSIS']], df_csf, on=['RID', 'VISCODE2'], how='inner')\n",
    "df_merged.dropna(inplace=True, axis=0) # remove entire row\n",
    "print_df_info(df_merged)\n",
    "\n",
    "# merge mri in df_merged\n",
    "df_merged = pd.merge(df_merged, df_mri, on=['RID', 'VISCODE'], how='inner')\n",
    "print_df_info(df_merged)\n",
    "\n",
    "# merge PET in df_merged\n",
    "df_merged = pd.merge(df_merged, df_pet, on=['RID', 'VISCODE'], how='inner')\n",
    "print_df_info(df_merged)\n",
    "\n",
    "print(df_merged.value_counts('DIAGNOSIS'))\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Analisys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hist(stat_type = 'count', palette = 'pastel'):\n",
    "    # Step 1: Define the custom order and labels\n",
    "    class_order = [1.0, 2.0, 3.0]\n",
    "    class_labels = ['HS', 'MCI', 'AD']\n",
    "    \n",
    "    sep = ''\n",
    "    if (stat_type == 'percent'):\n",
    "        sep = '%'\n",
    "\n",
    "    # Step 2: Plot the frequency graph with custom labels\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.countplot(x='DIAGNOSIS', data=df_merged, hue='DIAGNOSIS', order=class_order, palette=palette, stat=stat_type, legend='full')\n",
    "\n",
    "    # Step 3: Add the exact count values on top of the bars\n",
    "    for p, label in zip(ax.patches, class_labels):\n",
    "        ax.annotate(f'{int(p.get_height())}' + sep, \n",
    "                    (p.get_x() + p.get_width() / 2.0, p.get_height()), \n",
    "                    ha='center', va='baseline', \n",
    "                    fontsize=12, color='black', xytext=(0, 5), \n",
    "                    textcoords='offset points')\n",
    "\n",
    "    # Update x-tick labels to use the custom labels\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    \n",
    "    plt.xlabel('Diagnosis Class')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequency of Diagnosis Classes')\n",
    "    plt.show()\n",
    "\n",
    "print_hist(stat_type = 'count', palette = 'flare')\n",
    "print_hist(stat_type = 'percent', palette = 'flare')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_merged.iloc[:, 5:]\n",
    "Y = df_merged[['DIAGNOSIS']]\n",
    "\n",
    "print_df_info(X)\n",
    "print_df_info(Y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce Data Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "svm = SVC(C=1.0, decision_function_shape='ovr', kernel='rbf', random_state=state)\n",
    "sfs = SequentialFeatureSelector(svm, n_features_to_select=10, scoring=make_scorer(f1_score , average='macro'), cv=10, n_jobs=-1)\n",
    "X_reduced = sfs.fit_transform(X_scaled, Y.values.ravel())\n",
    "\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the selected features\n",
    "selected_features_indices = sfs.get_support(indices=True)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features_names = X.columns[selected_features_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(\"Selected features:\", selected_features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canonical Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cca = CCA(n_components=1)\n",
    "print(X_scaled.shape)\n",
    "\n",
    "X_reduced = cca.fit(X_scaled, Y.values.ravel()).transform(X_scaled)\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(np.round(cca.coef_, 2), columns = [X.columns]).T\n",
    "print(coef_df)\n",
    "print(coef_df.columns[0])\n",
    "\n",
    "print(coef_df.nlargest(10, columns=coef_df.columns[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Oversampling\n",
    "\n",
    "Imbalanced Data: If your data is imbalanced, meaning one class is much more prevalent than the others, the model might be overfitting to the dominant class. This could lead to high precision for that class but poor generalization.\n",
    "\n",
    "Increase the number of samples in the minority class using techniques like SMOTE (Synthetic Minority Over-sampling Technique) or simple random oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled = X_reduced\n",
    "Y_resampled = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTETomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sampling using SMOTE and cleaning using Tomek links.\n",
    "# Combine over- and under-sampling using SMOTE and Tomek links.\n",
    "from imblearn.under_sampling import *\n",
    "\n",
    "oversample = SMOTETomek(n_jobs=-1, random_state=state)\n",
    "X_resampled, Y_resampled = oversample.fit_resample(X_reduced, Y)\n",
    "\n",
    "print(X_reduced.shape)\n",
    "print(Y.shape)\n",
    "print(Y.value_counts())\n",
    "print()\n",
    "\n",
    "print(X_resampled.shape)\n",
    "print(Y_resampled.shape)\n",
    "print(Y_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [1e-3, 1e-2, 1e-1, 1.0, 1e1, 1e2],\n",
    "    'decision_function_shape': ['ovo', 'ovr']\n",
    "    #'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# target metric\n",
    "target_score = make_scorer(f1_score , average='macro')\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, \n",
    "                           scoring=target_score, cv=10, verbose=1, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_resampled, Y_resampled.values.ravel())\n",
    "\n",
    "print(round(grid_search.best_score_ * 100.0, 2))\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(Y.value_counts())\n",
    "print(Y_resampled.value_counts())\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_resampled, Y_resampled.values.ravel(), test_size=0.30, random_state=42)\n",
    "svm = SVC(C=1.0, class_weight='balanced', decision_function_shape='ovr', kernel='rbf')\n",
    "svm.fit(x_train, y_train)\n",
    "y_pred = svm.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_transformed = Y_resampled.values.ravel()\n",
    "\n",
    "def train_with_test_split(clf, test_size = 0.30, random_state = state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, Y_transformed, test_size=test_size, random_state=random_state)\n",
    "    scores = []\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    scores.append((y_test, y_pred))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def train_with_cross_validation(clf, kfolds = 10, suffle = True, random_state = state):\n",
    "    skf = StratifiedKFold(n_splits=kfolds, shuffle=suffle, random_state=random_state)\n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_resampled, Y_transformed):\n",
    "        x_train_fold, x_test_fold = X_resampled[train_index], X_resampled[test_index]\n",
    "        y_train_fold, y_test_fold = Y_transformed[train_index], Y_transformed[test_index]\n",
    "        clf.fit(x_train_fold, y_train_fold)\n",
    "        y_pred_fold = clf.predict(x_test_fold)\n",
    "        scores.append((y_test_fold, y_pred_fold))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_pred, pallete='flare'):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    classes = ['HS', 'MCI', 'AD']\n",
    "\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    cm_df = pd.DataFrame(cm_percentage, index=classes, columns=classes)\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm_df, annot=True, cmap=sns.color_palette(pallete, as_cmap=True), fmt='.2f')\n",
    "    \n",
    "    for t in plt.gca().texts:\n",
    "        t.set_text(t.get_text() + \"%\")\n",
    "    \n",
    "    plt.title('Matriz de Confusão (Porcentagem)')\n",
    "    plt.ylabel('Valores Reais')\n",
    "    plt.xlabel('Valores Previstos')\n",
    "    #plt.savefig('confusion-matrix.png', format='png', dpi=400, transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a, b):\n",
    "    return a / b if b != 0 else 0\n",
    "\n",
    "def generate_metrics_report(y_test, y_pred, n_classes = 3, target_map = {1: 'HS', 2: 'MCI', 3: 'AD'}):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    report_data = []\n",
    "\n",
    "    for c in range(n_classes):\n",
    "        tp = cm[c, c]\n",
    "        fp = sum(cm[:, c]) - cm[c, c]\n",
    "        fn = sum(cm[c, :]) - cm[c, c]\n",
    "        tn = sum(np.delete(sum(cm) - cm[c, :], c))\n",
    "\n",
    "        recall = safe_divide(tp, tp + fn)\n",
    "        precision = safe_divide(tp, tp + fp)\n",
    "        specificity = safe_divide(tn, tn + fp)\n",
    "        f1_score = safe_divide(2 * precision * recall, precision + recall)\n",
    "        fpr = safe_divide(fp, fp + tn)\n",
    "        fnr = safe_divide(fn, fn + tp)\n",
    "        acc = safe_divide(tp + tn, tp + tn + fp + fn) \n",
    "            \n",
    "        report_data.append([target_map[c + 1], round(acc, 4), round(precision, 4), round(f1_score, 4), round(recall, 4), round(specificity, 4), round(fpr, 4), round(fnr, 4)])\n",
    "\n",
    "    df_report = pd.DataFrame(report_data, columns = ['class', 'accuracy', 'precision', 'f1-score', 'sensitivity', 'specificity', 'fpr', 'fnr'])\n",
    "     \n",
    "    avg_row = {\n",
    "        'class': 'Avg',\n",
    "        'accuracy': round(df_report['accuracy'].mean(), 4),\n",
    "        'precision': round(df_report['precision'].mean(), 4),\n",
    "        'f1-score': round(df_report['f1-score'].mean(), 4),\n",
    "        'sensitivity': round(df_report['sensitivity'].mean(), 4),\n",
    "        'specificity': round(df_report['specificity'].mean(), 4),\n",
    "        'fpr': round(df_report['fpr'].mean(), 4),\n",
    "        'fnr': round(df_report['fnr'].mean(), 4)\n",
    "    }\n",
    "\n",
    "    df_report.loc[-1] = [avg_row['class'], avg_row['accuracy'], avg_row['precision'], avg_row['f1-score'], avg_row['sensitivity'], avg_row['specificity'], avg_row['fpr'], avg_row['fnr']]\n",
    "    df_report.index = df_report.index + 1  # shifting index\n",
    "    \n",
    "    return df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_test, y_pred, score='accuracy', n_classes = 3, target_map = {1: 'HS', 2: 'MCI', 3: 'AD'}):\n",
    "    df_report = generate_metrics_report(y_test, y_pred, n_classes = n_classes, target_map = target_map)\n",
    "    return round(df_report[score].mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(clf, kfolds, score='accuracy', test_size = 0.30, fn_action = None, suffle=True, random_state=state):\n",
    "    if (kfolds == 1):\n",
    "        print(f'Running Simple TrainTestSplit with test size: {test_size}: ')\n",
    "    else:\n",
    "        print(f'Running Cross Validation with {kfolds} folds:')\n",
    "        \n",
    "    print(f'- clf: {clf}')\n",
    "    print(f'- score: {score}')\n",
    "    print(f'- suffle: {suffle}')\n",
    "    print(f'- random_state: {random_state}\\n')\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # we use simple train_test_split\n",
    "    if (kfolds == 1):\n",
    "        scores = train_with_test_split(clf, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        scores = train_with_cross_validation(clf, kfolds=kfolds, suffle=suffle, random_state=random_state)\n",
    "    \n",
    "    best_params = []\n",
    "        \n",
    "    # Step 1: Calculate the scores for all folds\n",
    "    scores_values = np.array([calculate_metrics(y_test, y_pred, n_classes=3, score=score) \n",
    "                             for y_test, y_pred in scores])\n",
    "\n",
    "    # Step 2: Calculate the average score\n",
    "    average_score = np.mean(scores_values)\n",
    "\n",
    "    # Step 3: Find the index of the score closest to the average score\n",
    "    closest_index = np.argmin(np.abs(scores_values - average_score))\n",
    "\n",
    "    # Retrieve the parameters corresponding to the closest score\n",
    "    best_params = scores[closest_index]\n",
    "                \n",
    "    df_metrics = generate_metrics_report(best_params[0], best_params[1], n_classes = 3)\n",
    "            \n",
    "    print('{} ± {} & {} ± {} & {} ± {} & {} ± {} & {} ± {} & {} ± {} & {} ± {}'\n",
    "          .format(round(df_metrics['accuracy'].mean() * 100.0, 2), \n",
    "                  round(df_metrics['accuracy'].std() * 100.0, 2),\n",
    "                  round(df_metrics['precision'].mean() * 100.0, 2), \n",
    "                  round(df_metrics['precision'].std() * 100.0, 2),\n",
    "                  round(df_metrics['f1-score'].mean() * 100.0, 2), \n",
    "                  round(df_metrics['f1-score'].std() * 100.0, 2),\n",
    "                  round(df_metrics['sensitivity'].mean() * 100.0, 2), \n",
    "                  round(df_metrics['sensitivity'].std() * 100.0, 2),\n",
    "                  round(df_metrics['specificity'].mean() * 100.0, 2), \n",
    "                  round(df_metrics['specificity'].std() * 100.0, 2),\n",
    "                  round(df_metrics['fpr'].mean() * 100.0, 2), \n",
    "                  round(df_metrics['fpr'].std() * 100.0, 2),\n",
    "                  round(df_metrics['fnr'].mean() * 100.0, 2),\n",
    "                  round(df_metrics['fnr'].std() * 100.0, 2)))\n",
    "    \n",
    "    print(f'[+] Best value (folds) ({score}): {round(scores_values[closest_index], 4) * 100.0}% (+- {round(scores_values.std() * 100.0, 2)}%)')\n",
    "    print(f'[+] Accuracy Mean: {round(df_metrics['accuracy'].mean() * 100.0, 2)}% (+- {round(df_metrics['accuracy'].std() * 100.0, 2)}%)')\n",
    "    print(f'[+] Precision Mean: {round(df_metrics['precision'].mean() * 100.0, 2)}% (+- {round(df_metrics['precision'].std() * 100.0, 2)}%)')\n",
    "    print(f'[+] F1-Score Mean: {round(df_metrics['f1-score'].mean() * 100.0, 2)}% (+- {round(df_metrics['f1-score'].std() * 100.0, 2)}%)')\n",
    "    print(f'[+] Sensitivity Mean: {round(df_metrics['sensitivity'].mean() * 100.0, 2)}% (+- {round(df_metrics['sensitivity'].std() * 100.0, 2)}%)')\n",
    "    print(f'[+] Specificity Mean: {round(df_metrics['specificity'].mean() * 100.0, 2)}% (+- {round(df_metrics['specificity'].std() * 100.0, 2)}%)')\n",
    "    print(f'[+] False Positive Rate Mean: {round(df_metrics['fpr'].mean() * 100.0, 2)}% (+- {round(df_metrics['fpr'].std() * 100.0, 2)}%)')\n",
    "    print(f'[+] False Negative Rate Mean: {round(df_metrics['fnr'].mean() * 100.0, 2)}% (+- {round(df_metrics['fnr'].std() * 100.0, 2)}%)')\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print(df_metrics)\n",
    "    print()\n",
    "    \n",
    "    if (fn_action != None):\n",
    "        fn_action(best_params[0], best_params[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(SVC(C=1.0, kernel='rbf', random_state=state))\n",
    "print_metrics(clf, 10, score='f1-score', suffle=True, random_state=state)\n",
    "\n",
    "clf = OneVsOneClassifier(SVC(C=1.0, kernel='rbf', random_state=state))\n",
    "print_metrics(clf, 10, score='f1-score', suffle=True, random_state=state, fn_action=plot_confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
